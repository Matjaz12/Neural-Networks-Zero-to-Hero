{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Micrograd\n",
        "\n",
        "1. watch the [micrograd video](https://www.youtube.com/watch?v=VMj-3S1tku0) on YouTube\n",
        "2. come back and complete these exercises to level up :)"
      ],
      "metadata": {
        "id": "JnGHatCI51JP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. *Write the function df that returns the analytical gradient of f, i.e. use your skills from calculus to take the derivative, then implement the formula.*"
      ],
      "metadata": {
        "id": "OFt6NKOz6iBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here is a mathematical expression that takes 3 inputs and produces one output\n",
        "from math import sin, cos\n",
        "\n",
        "def f(a, b, c):\n",
        "  return -a**3 + sin(3*b) - 1.0/c + b**2.5 - a**0.5\n",
        "\n",
        "print(f(2, 3, 4))"
      ],
      "metadata": {
        "id": "3Jx9fCXl5xHd",
        "outputId": "36fa4786-af90-4e72-ff9e-9777fcc9cc89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.336362190988558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------\n",
        "# TODO\n",
        "def gradf(a, b, c):\n",
        "    dfda = -3 * a ** 2 - 1 / 2 * a ** (-1 / 2)\n",
        "    dfdb = 3 * cos(3 * b) + 5 / 2 * b ** (3 / 2)\n",
        "    dfdc = c ** (-2)\n",
        "\n",
        "    grad = [dfda, dfdb, dfdc]\n",
        "    return grad\n",
        "# ------"
      ],
      "metadata": {
        "id": "qXaH59eL9zxf"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# expected answer is the list of \n",
        "ans = [-12.353553390593273, 10.25699027111255, 0.0625]\n",
        "yours = gradf(2, 3, 4)\n",
        "for dim in range(3):\n",
        "  ok = 'OK' if abs(yours[dim] - ans[dim]) < 1e-5 else 'WRONG!'\n",
        "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {yours[dim]}\")\n"
      ],
      "metadata": {
        "id": "-MyF3Seg0v1W",
        "outputId": "0888ee52-0f71-495e-9c55-eabcf3810262",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK for dim 0: expected -12.353553390593273, yours returns -12.353553390593273\n",
            "OK for dim 1: expected 10.25699027111255, yours returns 10.25699027111255\n",
            "OK for dim 2: expected 0.0625, yours returns 0.0625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. *Now estimate the gradient numerically without any calculus, using the approximation we used in the video. you should not call the function df from the last cell.*"
      ],
      "metadata": {
        "id": "7u0-_nLcH0ZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------\n",
        "# TODO\n",
        "def gradf_numeric(a, b, c, h = 1e-6):\n",
        "    dfda = (f(a + h, b, c) - f(a, b, c)) / h\n",
        "    dfdb = (f(a, b + h, c) - f(a, b, c)) / h\n",
        "    dfdc = (f(a, b, c + h) - f(a, b, c)) / h\n",
        "\n",
        "    grad = [dfda, dfdb, dfdc]\n",
        "    return grad\n",
        "\n",
        "numerical_grad = gradf_numeric(2, 3, 4)\n",
        "# ------"
      ],
      "metadata": {
        "id": "_27n-KTA9Qla"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dim in range(3):\n",
        "  ok = 'OK' if abs(numerical_grad[dim] - ans[dim]) < 1e-5 else 'WRONG!'\n",
        "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {numerical_grad[dim]}\")"
      ],
      "metadata": {
        "id": "hKqnIY8g04aH",
        "outputId": "7583a365-f6b6-4cee-f3bf-b2068741ca88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK for dim 0: expected -12.353553390593273, yours returns -12.353559348809995\n",
            "OK for dim 1: expected 10.25699027111255, yours returns 10.256991666679482\n",
            "OK for dim 2: expected 0.0625, yours returns 0.062499984743169534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. *There is an alternative formula that provides a much better numerical  approximation to the derivative of a function. Learn about it here: https://en.wikipedia.org/wiki/Symmetric_derivative implement it. confirm that for the same step size h this version gives a better approximation.*"
      ],
      "metadata": {
        "id": "mHoz2CuwH7qG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------\n",
        "# TODO\n",
        "def gradf_numeric2(a, b, c, h = 1e-6):\n",
        "    dfda = (f(a + h, b, c) - f(a - h, b, c)) / (2 * h)\n",
        "    dfdb = (f(a, b + h, c) - f(a, b - h, c)) / (2 * h)\n",
        "    dfdc = (f(a, b, c + h) - f(a, b, c - h)) / (2 * h)\n",
        "\n",
        "    grad = [dfda, dfdb, dfdc]\n",
        "    return grad\n",
        "\n",
        "numerical_grad2 = gradf_numeric2(2, 3, 4)\n",
        "# ------"
      ],
      "metadata": {
        "id": "C3NiVC5a2FnG"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dim in range(3):\n",
        "  ok = 'OK' if abs(numerical_grad2[dim] - ans[dim]) < 1e-5 else 'WRONG!'\n",
        "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {numerical_grad2[dim]}\")"
      ],
      "metadata": {
        "id": "xFxfKnsA2KLz",
        "outputId": "ad6fe1aa-85a0-41bb-e171-8aeb07e90eca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK for dim 0: expected -12.353553390593273, yours returns -12.353553391353245\n",
            "OK for dim 1: expected 10.25699027111255, yours returns 10.25699027401572\n",
            "OK for dim 2: expected 0.0625, yours returns 0.06250000028629188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. *Without referencing our code/video __too__ much, make this cell work you'll have to implement (in some cases re-implemented) a number of functions of the Value object, similar to what we've seen in the video. Instead of the squared error loss this implements the negative log likelihood loss, which is very often used in classification.*\n"
      ],
      "metadata": {
        "id": "tklF9s_4AtlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Value class starter code, with many functions taken out\n",
        "from math import exp, log\n",
        "\n",
        "class Value:\n",
        "    def __init__(self, data, _children=(), _op='', label=''):\n",
        "        self.data = data\n",
        "        self.grad = 0.0\n",
        "        self._backward = lambda: None\n",
        "        self._prev = set(_children)\n",
        "        self._op = _op\n",
        "        self.label = label\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Value(data={self.data})\"\n",
        "  \n",
        "    def __add__(self, other): # exactly as in the video\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data + other.data, (self, other), '+')\n",
        "    \n",
        "        def _backward():\n",
        "            self.grad += 1.0 * out.grad\n",
        "            other.grad += 1.0 * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    # ------\n",
        "    # TODO\n",
        "    # re-implement all the other functions needed for the exercises below\n",
        "    # your code here\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data * other.data, (self, other), \"*\")\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += other.data * out.grad\n",
        "            other.grad += self.data * out.grad\n",
        "\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    def __pow__(self, other):\n",
        "        assert isinstance(other, (int, float)), \"only supporting int / float powers\"\n",
        "        x = self.data\n",
        "        out = Value(self.data ** (other), (self, ), f\"**{other}\")\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += other * (self.data ** (other - 1)) * out.grad\n",
        "\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "\n",
        "    def exp(self):\n",
        "        x = self.data\n",
        "\n",
        "        out = Value(exp(x), (self, ), \"exp\")\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += out.data * out.grad\n",
        "\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "\n",
        "    def log(self):\n",
        "        x = self.data\n",
        "        out = Value(log(x), (self, ), 'log')\n",
        "        def _backward():\n",
        "            self.grad += (x **-1) * out.grad\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "\n",
        "    def tanh(self):\n",
        "        x = self.data\n",
        "        t = (exp(2*x) - 1)/(exp(2*x) + 1)\n",
        "        out = Value(t, (self, ), 'tanh')\n",
        "        \n",
        "        def _backward():\n",
        "          self.grad += (1 - t**2) * out.grad\n",
        "        out._backward = _backward\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __rmul__(self, other): # other * self\n",
        "        return self * other\n",
        "\n",
        "    def __truediv__(self, other):\n",
        "        return self * other**-1\n",
        "\n",
        "    def __neg__(self):\n",
        "        return self *-1\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        return self + (-other)\n",
        "\n",
        "    def __radd__(self, other): \n",
        "        return self + other\n",
        "    \n",
        "\n",
        "    def sin(self):\n",
        "        out = Value(sin(self.data), (self, ), 'sin')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += cos(self.data) * out.grad\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    # ------\n",
        "\n",
        "    def backward(self):\n",
        "        topo = []\n",
        "        visited = set()\n",
        "\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "\n",
        "                for child_v in v._prev:\n",
        "                    build_topo(child_v)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "\n",
        "        self.grad = 1.0\n",
        "        for node in reversed(topo):\n",
        "            node._backward()"
      ],
      "metadata": {
        "id": "nAPe_RVrCTeO"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is the softmax function\n",
        "# https://en.wikipedia.org/wiki/Softmax_function\n",
        "def softmax(logits):\n",
        "    counts = [logit.exp() for logit in logits]\n",
        "    denominator = sum(counts)\n",
        "    out = [c / denominator for c in counts]\n",
        "    return out"
      ],
      "metadata": {
        "id": "VgWvwVQNAvnI"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is the negative log likelihood loss function, pervasive in classification\n",
        "logits = [Value(0.0), Value(3.0), Value(-2.0), Value(1.0)]\n",
        "probs = softmax(logits)\n",
        "print(f\"probs: {[round(p.data, 5) for p in probs]}\")\n",
        "\n",
        "loss = -probs[3].log() # dim 3 acts as the label for this input example\n",
        "loss.backward()\n",
        "print(loss.data)"
      ],
      "metadata": {
        "id": "dF1pIvapAn2M",
        "outputId": "70847d17-6e43-4b30-9734-58a7312ebd25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probs: [0.04177, 0.83902, 0.00565, 0.11355]\n",
            "2.1755153626167147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ans = [0.041772570515350445, 0.8390245074625319, 0.005653302662216329, -0.8864503806400986]\n",
        "for dim in range(4):\n",
        "  ok = 'OK' if abs(logits[dim].grad - ans[dim]) < 1e-5 else 'WRONG!'\n",
        "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {logits[dim].grad}\")"
      ],
      "metadata": {
        "id": "_Q1vSoCk2ooZ",
        "outputId": "29d52f50-7b5e-45bd-e698-9e3b113e916b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK for dim 0: expected 0.041772570515350445, yours returns 0.041772570515350445\n",
            "OK for dim 1: expected 0.8390245074625319, yours returns 0.8390245074625319\n",
            "OK for dim 2: expected 0.005653302662216329, yours returns 0.005653302662216329\n",
            "OK for dim 3: expected -0.8864503806400986, yours returns -0.8864503806400986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. *Verify the gradient using the torch library, torch should give you the exact same gradient*"
      ],
      "metadata": {
        "id": "0ynZBLbzIhJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "logits = torch.tensor([[0.0, 3.0, -2.0, 1.0]], requires_grad=True)\n",
        "probs = torch.softmax(logits, dim=1)[0]\n",
        "print(f\"probs: {probs}\")\n",
        "loss = -probs[3].log()\n",
        "loss.backward()\n",
        "\n",
        "print(f\"logits.grad: {logits.grad}\")"
      ],
      "metadata": {
        "id": "q7ca1SVAGG1S",
        "outputId": "94edbbc5-2a2b-4cb7-9a65-6d1480af3b64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probs: tensor([0.0418, 0.8390, 0.0057, 0.1135], grad_fn=<SelectBackward0>)\n",
            "logits.grad: tensor([[ 0.0418,  0.8390,  0.0057, -0.8865]])\n"
          ]
        }
      ]
    }
  ]
}